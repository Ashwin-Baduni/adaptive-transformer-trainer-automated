# Adaptive Transformer Trainer (Automated) ğŸš€

An intelligent PyTorch Transformer implementation from scratch with automated training, memory optimization, and dataset-aware preprocessing. This project converts notebook-based transformer training into production-ready Python scripts with smart GPU memory management and adaptive training strategies.

## ğŸŒŸ Key Features

- **ğŸ§  Smart Memory Management**: Automatically detects GPU memory and adjusts parameters accordingly
- **ğŸ“Š Dataset Intelligence**: Handles variable-length sequences through intelligent truncation
- **ğŸ”„ Automated Training Pipeline**: One-command training with progressive fallback strategies
- **ğŸ’¾ Low-Memory GPU Support**: Optimized for GPUs with 4GB+ memory (tested on GTX 1050)
- **ğŸ¯ Multiple Decoding Strategies**: Beam search, greedy decoding, and attention visualization
- **âš¡ Progressive Fallback**: GPU â†’ Reduced batch size â†’ CPU training

## ğŸ› ï¸ Project Structure

```
â”œâ”€â”€ config.py                    # Configuration and hyperparameters
â”œâ”€â”€ model.py                     # Transformer architecture implementation
â”œâ”€â”€ train.py                     # Core training loop
â”œâ”€â”€ dataset.py                   # Data loading with truncation support
â”œâ”€â”€ runner.py                    # Automated training with memory optimization
â”œâ”€â”€ translate.py                 # Translation and inference
â”œâ”€â”€ attention_visual.py          # Attention mechanism visualization
â”œâ”€â”€ Beam_Search.py               # Beam search implementation
â”œâ”€â”€ inference.py                 # Model inference utilities
â”œâ”€â”€ Local_Train.py               # Local training configuration
â”œâ”€â”€ memory_optimized_train.py    # Memory-optimized training scripts
â”œâ”€â”€ cpu_fallback_train.py        # CPU fallback training
â””â”€â”€ requirements.txt             # Dependencies
```

## ğŸš€ Quick Start

### Installation

```
git clone https://github.com/Ashwin-Baduni/adaptive-transformer-trainer-automated.git
cd adaptive-transformer-trainer-automated
pip install -r requirements.txt
```

### Training

- **Automatic training with memory optimization**
  ```
  python runner.py --seq-len 200 --batch-size 1 --epochs 20
  ```
- **Quick test (1 epoch)**
  ```
  python runner.py --seq-len 100 --batch-size 1 --epochs 1
  ```
- **CPU fallback if GPU memory insufficient**
  ```
  python runner.py --cpu-only --epochs 5
  ```

### Translation

```
python translate.py
# or
python inference.py
```

## ğŸ’¡ What Makes This Special

### ğŸ§  Intelligent Memory Management

- **Auto-Detection**: Automatically detects GPU memory (e.g., 3.94GB GTX 1050)
- **Dynamic Adjustment**: Adjusts batch size, sequence length, and model dimensions
- **Progressive Fallback**: GPU â†’ Smaller parameters â†’ CPU training

### ğŸ“Š Dataset-Aware Processing

- **Smart Truncation**: Handles sentences up to 309 tokens through intelligent truncation
- **Quality Preservation**: Maintains translation quality while fitting memory constraints
- **Flexible Data Sources**: Supports any language pair through Hugging Face datasets

### ğŸ”„ Automated Training Pipeline

- **One-Command Training**: Complete training setup with optimal configurations
- **Checkpoint Management**: Automatic saving and resuming from checkpoints
- **Monitoring Integration**: TensorBoard logging for training visualization

## ğŸ“ˆ Training Results

Successfully tested on GTX 1050 (4GB VRAM):

- **Dataset**: OPUS Books (English â†’ Italian, 32,332 pairs)
- **Training Speed**: ~14 iterations/second
- **Memory Efficiency**: Optimized for 4GB GPU memory
- **Sequence Handling**: 100-200 tokens (configurable with truncation)
- **Training Time**: ~34 minutes per epoch

## ğŸ¯ Use Cases

- **ğŸ”¬ Research**: Experiment with transformer architectures on limited hardware
- **ğŸ“š Education**: Learn transformer implementation with practical constraints
- **ğŸš€ Production**: Deploy on resource-constrained environments
- **ğŸ“Š Custom Data**: Train on your own translation datasets

## ğŸ”§ Configuration

### Basic Configuration

Modify `config.py` for different setups:

```
{
    'datasource': 'opus_books',     # Dataset source
    'lang_src': 'en',              # Source language
    'lang_tgt': 'it',              # Target language
    'seq_len': 200,                # Sequence length
    'batch_size': 1,               # Batch size
    'num_epochs': 20,              # Training epochs
    'd_model': 256,                # Model dimension
}
```

### Memory Optimization

The system automatically optimizes based on available GPU memory.

## ğŸ™ Acknowledgments

- **Based on the "Attention Is All You Need" paper**
- **Inspired by Hugging Face Transformers**
- **Optimized for educational and research purposes**
- **Built with PyTorch and modern ML best practices**

## ğŸ“ Contact

**Ashwin Baduni**  
Email: baduniashwin@gmail.com
